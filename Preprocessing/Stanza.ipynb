{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stanza.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn5A3ReeCgP6"
      },
      "source": [
        "## Stanza\r\n",
        "\r\n",
        "https://stanfordnlp.github.io/stanza/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u6Ylrw2_FKz"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFkmvfoc_bwB"
      },
      "source": [
        "import stanza"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DbHPtVv_yTn"
      },
      "source": [
        "stanza.download('en') # download English model\r\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline\r\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7UUEolhCH-o",
        "outputId": "147c6d5b-a8b0-4f99-a7d2-904e8dddc9f2"
      },
      "source": [
        "print(doc)\r\n",
        "print(doc.entities)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "  [\n",
            "    {\n",
            "      \"id\": 1,\n",
            "      \"text\": \"Barack\",\n",
            "      \"lemma\": \"Barack\",\n",
            "      \"upos\": \"PROPN\",\n",
            "      \"xpos\": \"NNP\",\n",
            "      \"feats\": \"Number=Sing\",\n",
            "      \"head\": 4,\n",
            "      \"deprel\": \"nsubj:pass\",\n",
            "      \"misc\": \"start_char=0|end_char=6\",\n",
            "      \"ner\": \"B-PERSON\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 2,\n",
            "      \"text\": \"Obama\",\n",
            "      \"lemma\": \"Obama\",\n",
            "      \"upos\": \"PROPN\",\n",
            "      \"xpos\": \"NNP\",\n",
            "      \"feats\": \"Number=Sing\",\n",
            "      \"head\": 1,\n",
            "      \"deprel\": \"flat\",\n",
            "      \"misc\": \"start_char=7|end_char=12\",\n",
            "      \"ner\": \"E-PERSON\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 3,\n",
            "      \"text\": \"was\",\n",
            "      \"lemma\": \"be\",\n",
            "      \"upos\": \"AUX\",\n",
            "      \"xpos\": \"VBD\",\n",
            "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "      \"head\": 4,\n",
            "      \"deprel\": \"aux:pass\",\n",
            "      \"misc\": \"start_char=13|end_char=16\",\n",
            "      \"ner\": \"O\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 4,\n",
            "      \"text\": \"born\",\n",
            "      \"lemma\": \"bear\",\n",
            "      \"upos\": \"VERB\",\n",
            "      \"xpos\": \"VBN\",\n",
            "      \"feats\": \"Tense=Past|VerbForm=Part|Voice=Pass\",\n",
            "      \"head\": 0,\n",
            "      \"deprel\": \"root\",\n",
            "      \"misc\": \"start_char=17|end_char=21\",\n",
            "      \"ner\": \"O\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 5,\n",
            "      \"text\": \"in\",\n",
            "      \"lemma\": \"in\",\n",
            "      \"upos\": \"ADP\",\n",
            "      \"xpos\": \"IN\",\n",
            "      \"head\": 6,\n",
            "      \"deprel\": \"case\",\n",
            "      \"misc\": \"start_char=22|end_char=24\",\n",
            "      \"ner\": \"O\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 6,\n",
            "      \"text\": \"Hawaii\",\n",
            "      \"lemma\": \"Hawaii\",\n",
            "      \"upos\": \"PROPN\",\n",
            "      \"xpos\": \"NNP\",\n",
            "      \"feats\": \"Number=Sing\",\n",
            "      \"head\": 4,\n",
            "      \"deprel\": \"obl\",\n",
            "      \"misc\": \"start_char=25|end_char=31\",\n",
            "      \"ner\": \"S-GPE\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 7,\n",
            "      \"text\": \".\",\n",
            "      \"lemma\": \".\",\n",
            "      \"upos\": \"PUNCT\",\n",
            "      \"xpos\": \".\",\n",
            "      \"head\": 4,\n",
            "      \"deprel\": \"punct\",\n",
            "      \"misc\": \"start_char=31|end_char=32\",\n",
            "      \"ner\": \"O\"\n",
            "    }\n",
            "  ]\n",
            "]\n",
            "[{\n",
            "  \"text\": \"Barack Obama\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 0,\n",
            "  \"end_char\": 12\n",
            "}, {\n",
            "  \"text\": \"Hawaii\",\n",
            "  \"type\": \"GPE\",\n",
            "  \"start_char\": 25,\n",
            "  \"end_char\": 31\n",
            "}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obeX6JhyElvg"
      },
      "source": [
        "#### Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zLPPHCoEN3Q",
        "outputId": "0a939bca-5438-4c6e-a904-164f6c25f955"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize')\r\n",
        "doc = nlp('This is a test sentence for stanza. This is another sentence.')\r\n",
        "for i, sentence in enumerate(doc.sentences):\r\n",
        "    print(f'====== Sentence {i+1} tokens =======')\r\n",
        "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-11 15:30:41 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "=======================\n",
            "\n",
            "2021-01-11 15:30:41 INFO: Use device: cpu\n",
            "2021-01-11 15:30:41 INFO: Loading: tokenize\n",
            "2021-01-11 15:30:41 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====== Sentence 1 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: a\n",
            "id: (4,)\ttext: test\n",
            "id: (5,)\ttext: sentence\n",
            "id: (6,)\ttext: for\n",
            "id: (7,)\ttext: stanza\n",
            "id: (8,)\ttext: .\n",
            "====== Sentence 2 tokens =======\n",
            "id: (1,)\ttext: This\n",
            "id: (2,)\ttext: is\n",
            "id: (3,)\ttext: another\n",
            "id: (4,)\ttext: sentence\n",
            "id: (5,)\ttext: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gW70fOFt1A"
      },
      "source": [
        "#### Multi-Word Token (MWT) Expansion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zQ6UZHeFJ9U",
        "outputId": "31a85b25-563d-44cc-c0f1-14fd78c08c93"
      },
      "source": [
        "stanza.download('fr') # download French model\r\n",
        "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt')\r\n",
        "doc = nlp('Nous avons atteint la fin du sentier.')\r\n",
        "for token in doc.sentences[0].tokens:\r\n",
        "    print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 18.8MB/s]                    \n",
            "2021-01-11 15:34:51 INFO: Downloading default packages for language: fr (French)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/fr/default.zip: 100%|██████████| 589M/589M [00:42<00:00, 13.7MB/s]\n",
            "2021-01-11 15:35:45 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2021-01-11 15:35:45 INFO: Loading these models for language: fr (French):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | gsd     |\n",
            "| mwt       | gsd     |\n",
            "=======================\n",
            "\n",
            "2021-01-11 15:35:45 INFO: Use device: cpu\n",
            "2021-01-11 15:35:45 INFO: Loading: tokenize\n",
            "2021-01-11 15:35:45 INFO: Loading: mwt\n",
            "2021-01-11 15:35:45 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "token: Nous\twords: Nous\n",
            "token: avons\twords: avons\n",
            "token: atteint\twords: atteint\n",
            "token: la\twords: la\n",
            "token: fin\twords: fin\n",
            "token: du\twords: de, le\n",
            "token: sentier\twords: sentier\n",
            "token: .\twords: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzHzGqE5GC1U",
        "outputId": "2bee12bb-013e-447d-e21a-ce5fbc1bfce3"
      },
      "source": [
        "for word in doc.sentences[0].words:\r\n",
        "    print(f'word: {word.text}\\tparent token: {word.parent.text}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word: Nous\tparent token: Nous\n",
            "word: avons\tparent token: avons\n",
            "word: atteint\tparent token: atteint\n",
            "word: la\tparent token: la\n",
            "word: fin\tparent token: fin\n",
            "word: de\tparent token: du\n",
            "word: le\tparent token: du\n",
            "word: sentier\tparent token: sentier\n",
            "word: .\tparent token: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBGD2nJ6Gfhg"
      },
      "source": [
        "#### POS\r\n",
        "\r\n",
        "universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW_42jgHGVew",
        "outputId": "58a8ab44-95fb-4b37-f80f-275dbf87ccee"
      },
      "source": [
        "doc = nlp('Barack Obama was born in Hawaii.')\r\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word: Barack\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
            "word: Obama\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
            "word: was\tupos: AUX\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            "word: born\tupos: VERB\txpos: VBN\tfeats: Tense=Past|VerbForm=Part|Voice=Pass\n",
            "word: in\tupos: ADP\txpos: IN\tfeats: _\n",
            "word: Hawaii\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
            "word: .\tupos: PUNCT\txpos: .\tfeats: _\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioC6EgHiHU43"
      },
      "source": [
        "#### Лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmnOYREaG8t-",
        "outputId": "32d0ce5f-fae3-4926-ed55-cf9f196f2268"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\r\n",
        "doc = nlp('Barack Obama was born in Hawaii.')\r\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-11 15:42:37 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2021-01-11 15:42:37 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "=======================\n",
            "\n",
            "2021-01-11 15:42:37 INFO: Use device: cpu\n",
            "2021-01-11 15:42:37 INFO: Loading: tokenize\n",
            "2021-01-11 15:42:37 INFO: Loading: pos\n",
            "2021-01-11 15:42:39 INFO: Loading: lemma\n",
            "2021-01-11 15:42:39 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: was \tlemma: be\n",
            "word: born \tlemma: bear\n",
            "word: in \tlemma: in\n",
            "word: Hawaii \tlemma: Hawaii\n",
            "word: . \tlemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t1FtotIHtZv"
      },
      "source": [
        "#### Dependency Parsing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl6ozCKOHlaS",
        "outputId": "065a4c28-684e-4914-b03c-15f807ddf50a"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma,depparse')\r\n",
        "doc = nlp('Nous avons atteint la fin du sentier.')\r\n",
        "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-11 15:45:38 INFO: Loading these models for language: fr (French):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | gsd     |\n",
            "| mwt       | gsd     |\n",
            "| pos       | gsd     |\n",
            "| lemma     | gsd     |\n",
            "| depparse  | gsd     |\n",
            "=======================\n",
            "\n",
            "2021-01-11 15:45:38 INFO: Use device: cpu\n",
            "2021-01-11 15:45:38 INFO: Loading: tokenize\n",
            "2021-01-11 15:45:38 INFO: Loading: mwt\n",
            "2021-01-11 15:45:38 INFO: Loading: pos\n",
            "2021-01-11 15:45:40 INFO: Loading: lemma\n",
            "2021-01-11 15:45:40 INFO: Loading: depparse\n",
            "2021-01-11 15:45:41 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "id: 1\tword: Nous\thead id: 3\thead: atteint\tdeprel: nsubj\n",
            "id: 2\tword: avons\thead id: 3\thead: atteint\tdeprel: aux:tense\n",
            "id: 3\tword: atteint\thead id: 0\thead: root\tdeprel: root\n",
            "id: 4\tword: la\thead id: 5\thead: fin\tdeprel: det\n",
            "id: 5\tword: fin\thead id: 3\thead: atteint\tdeprel: obj\n",
            "id: 6\tword: de\thead id: 8\thead: sentier\tdeprel: case\n",
            "id: 7\tword: le\thead id: 8\thead: sentier\tdeprel: det\n",
            "id: 8\tword: sentier\thead id: 5\thead: fin\tdeprel: nmod\n",
            "id: 9\tword: .\thead id: 3\thead: atteint\tdeprel: punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVYaMVbsKEFc",
        "outputId": "94711d1d-e2df-4fd5-9e74-e39c91304226"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')\r\n",
        "doc = nlp('Colorless green ideas sleep furiously.')\r\n",
        "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-11 15:56:06 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2021-01-11 15:56:06 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| depparse  | ewt     |\n",
            "=======================\n",
            "\n",
            "2021-01-11 15:56:06 INFO: Use device: cpu\n",
            "2021-01-11 15:56:06 INFO: Loading: tokenize\n",
            "2021-01-11 15:56:06 INFO: Loading: pos\n",
            "2021-01-11 15:56:07 INFO: Loading: lemma\n",
            "2021-01-11 15:56:07 INFO: Loading: depparse\n",
            "2021-01-11 15:56:08 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "id: 1\tword: Colorless\thead id: 3\thead: ideas\tdeprel: amod\n",
            "id: 2\tword: green\thead id: 3\thead: ideas\tdeprel: amod\n",
            "id: 3\tword: ideas\thead id: 4\thead: sleep\tdeprel: nsubj\n",
            "id: 4\tword: sleep\thead id: 0\thead: root\tdeprel: root\n",
            "id: 5\tword: furiously\thead id: 4\thead: sleep\tdeprel: advmod\n",
            "id: 6\tword: .\thead id: 4\thead: sleep\tdeprel: punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE21y5KNIOt4"
      },
      "source": [
        "#### NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd6npMwXIQ_4",
        "outputId": "b83a7fa3-65ba-45fb-bd90-e97870839d3f"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\r\n",
        "doc = nlp(\"Twitter permanently suspends President Donald Trump.\")\r\n",
        "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-11 15:47:41 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2021-01-11 15:47:41 INFO: Use device: cpu\n",
            "2021-01-11 15:47:41 INFO: Loading: tokenize\n",
            "2021-01-11 15:47:41 INFO: Loading: ner\n",
            "2021-01-11 15:47:42 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "entity: Donald Trump\ttype: PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPGd3g7-I7vh"
      },
      "source": [
        "#### Sentiment Analysis\r\n",
        "\r\n",
        "negative (0), neutral (1), positive (2)\r\n",
        "\r\n",
        "Источник: https://arxiv.org/abs/1408.5882"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVsEEPUOIs1_",
        "outputId": "a446e5d7-b09f-4b73-d48a-10b76dfb0c62"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\r\n",
        "doc = nlp('He hates that they banned him. He is f** furious.')\r\n",
        "for i, sentence in enumerate(doc.sentences):\r\n",
        "    print(i, sentence.sentiment)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-11 15:53:04 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| sentiment | sstplus |\n",
            "=======================\n",
            "\n",
            "2021-01-11 15:53:04 INFO: Use device: cpu\n",
            "2021-01-11 15:53:04 INFO: Loading: tokenize\n",
            "2021-01-11 15:53:04 INFO: Loading: sentiment\n",
            "2021-01-11 15:53:06 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 1\n",
            "1 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}