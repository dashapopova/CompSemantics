{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-ftJodD62f"
      },
      "source": [
        "<h2><center>Препроцессинг</center></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z09EqOvoD623"
      },
      "source": [
        "### Препроцессинг \n",
        "([С использованием материалов Э.С. Клышинского](https://github.com/klyshinsky/ML_masters_2020/blob/master/Lecture_20201013_text_processing.ipynb))\n",
        "\n",
        "* Токенизация - выделение абзацев, предложений, токенов\n",
        "* Морфологический анализ (стемминг, лемматизация)\n",
        "* Синтаксический анализ - определение связей между словами (деревья зависимостей) или синтаксически связанных групп слов (деревья составляющих)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPnXNh5jD626"
      },
      "source": [
        "Поиграем с [файлом с новостями](https://github.com/klyshinsky/ML_masters_2020/blob/master/data/lenta2018_summer2.txt) с сайта http://lenta.ru/. Все новости отделены друг от друга пятью знаками равно, дальше идет дата новости, пять минусов, текст новости.\n",
        "\n",
        "Начнем с того, что загрузим новости в DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBxxq0QzD629"
      },
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lGxQhxlD63A"
      },
      "source": [
        "with open(\"/content/lenta\", encoding=\"utf-8\") as newsfile:\n",
        "    text_news = [(n.split(\"-----\\n\")[0].split('\\n')[0], \n",
        "                  n.split(\"-----\\n\")[0].split('\\n')[1], \n",
        "                  n.split(\"-----\\n\")[1]) for n in newsfile.read().split(\"=====\\n\")[1:]]\n",
        "    news = pd.DataFrame(text_news, columns = ['Header', 'Date', 'News'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op67jNsnD63C",
        "outputId": "f92cffba-72a5-4be8-8aa0-5f0a7beef46f"
      },
      "source": [
        "!head /content/lenta"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====\n",
            "«Королева дерьмовых роботов» перенесла операцию на мозге\n",
            "2018/06/01\n",
            "-----\n",
            "Шведская YouTube-знаменитость Симона Герц (Simone Giertz), прославившаяся необычными изобретениями, перенесла операцию. Об этом рассказали ее представители в Twitter. Еще в конце апреля девушка сообщила подписчикам о недуге. Оказалось, что у нее в мозге обнаружили доброкачественную опухоль размером с мяч для гольфа. Блогер даже дала ей имя, назвав новообразование Брайаном.  Герц сильно переживала перед хирургическим вмешательством. В одном из предоперационных видео она говорила, что очень напугана. Однако врачи быстро удалили опухоль и спасли девушке жизнь. «Симона вышла из хирургии, и ее врачи очень довольны тем, как все прошло. Она проспала достаточно долго, чтобы сделать неуместную шутку, так что все хорошо», — говорится на ее странице в Twitter.  27-летняя изобретательница из Стокгольма прославилась несколько лет назад, показывая подписчикам свои бесполезные изобретения. Например, Герц демонстрировала самодельный будильник, к которому приделана резиновая рука, избивающая спящего человека. Кроме того, девушка создавала автоматизированную машину для наливания молока в тарелку с хлопьями. Сама Герц называет себя «королевой дерьмовых роботов», на ее канал подписаны более миллиона пользователей.\n",
            "=====\n",
            "У Су-57 нашли «тайные ноу-хау»\n",
            "2018/06/01\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RkGUWx19D63F",
        "outputId": "ee19f0b7-e177-47c1-d5b4-2ffad707d3a6"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«Королева дерьмовых роботов» перенесла операци...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Шведская YouTube-знаменитость Симона Герц (Sim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>У Су-57 нашли «тайные ноу-хау»</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Перспективный российский многофункциональный и...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Знарок лишился еще одного поста</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Санкт-петербургский клуб Континентальной хокке...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>СК заинтересовался задержанием аутиста московс...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Следственный комитет по Москве начал проверку ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Киевляне перекрыли проспект из-за сбитого поли...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Более 100 жителей Киева перекрыли проспект Гри...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Header  ...                                               News\n",
              "0  «Королева дерьмовых роботов» перенесла операци...  ...  Шведская YouTube-знаменитость Симона Герц (Sim...\n",
              "1                     У Су-57 нашли «тайные ноу-хау»  ...  Перспективный российский многофункциональный и...\n",
              "2                    Знарок лишился еще одного поста  ...  Санкт-петербургский клуб Континентальной хокке...\n",
              "3  СК заинтересовался задержанием аутиста московс...  ...  Следственный комитет по Москве начал проверку ...\n",
              "4  Киевляне перекрыли проспект из-за сбитого поли...  ...  Более 100 жителей Киева перекрыли проспект Гри...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gx9jC13D63K"
      },
      "source": [
        "Посмотрим, какие слова чаще всего встречаются в новостях. \n",
        "\n",
        "Разделим текст на слова простейшим образом, с помощью регулярных выражений: будем считать, что слово - это много русских или латинских букв."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzewyFpdD63L",
        "outputId": "4e89cba6-6a44-46c4-9d77-5b8a1849a53b"
      },
      "source": [
        "words = re.findall('[A-Za-zА-ЯЁа-яё]+-[A-Za-zА-ЯЁа-яё]+|[A-Za-zА-ЯЁа-яё]+', news.News.iloc[0])\n",
        "words"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Шведская',\n",
              " 'YouTube-знаменитость',\n",
              " 'Симона',\n",
              " 'Герц',\n",
              " 'Simone',\n",
              " 'Giertz',\n",
              " 'прославившаяся',\n",
              " 'необычными',\n",
              " 'изобретениями',\n",
              " 'перенесла',\n",
              " 'операцию',\n",
              " 'Об',\n",
              " 'этом',\n",
              " 'рассказали',\n",
              " 'ее',\n",
              " 'представители',\n",
              " 'в',\n",
              " 'Twitter',\n",
              " 'Еще',\n",
              " 'в',\n",
              " 'конце',\n",
              " 'апреля',\n",
              " 'девушка',\n",
              " 'сообщила',\n",
              " 'подписчикам',\n",
              " 'о',\n",
              " 'недуге',\n",
              " 'Оказалось',\n",
              " 'что',\n",
              " 'у',\n",
              " 'нее',\n",
              " 'в',\n",
              " 'мозге',\n",
              " 'обнаружили',\n",
              " 'доброкачественную',\n",
              " 'опухоль',\n",
              " 'размером',\n",
              " 'с',\n",
              " 'мяч',\n",
              " 'для',\n",
              " 'гольфа',\n",
              " 'Блогер',\n",
              " 'даже',\n",
              " 'дала',\n",
              " 'ей',\n",
              " 'имя',\n",
              " 'назвав',\n",
              " 'новообразование',\n",
              " 'Брайаном',\n",
              " 'Герц',\n",
              " 'сильно',\n",
              " 'переживала',\n",
              " 'перед',\n",
              " 'хирургическим',\n",
              " 'вмешательством',\n",
              " 'В',\n",
              " 'одном',\n",
              " 'из',\n",
              " 'предоперационных',\n",
              " 'видео',\n",
              " 'она',\n",
              " 'говорила',\n",
              " 'что',\n",
              " 'очень',\n",
              " 'напугана',\n",
              " 'Однако',\n",
              " 'врачи',\n",
              " 'быстро',\n",
              " 'удалили',\n",
              " 'опухоль',\n",
              " 'и',\n",
              " 'спасли',\n",
              " 'девушке',\n",
              " 'жизнь',\n",
              " 'Симона',\n",
              " 'вышла',\n",
              " 'из',\n",
              " 'хирургии',\n",
              " 'и',\n",
              " 'ее',\n",
              " 'врачи',\n",
              " 'очень',\n",
              " 'довольны',\n",
              " 'тем',\n",
              " 'как',\n",
              " 'все',\n",
              " 'прошло',\n",
              " 'Она',\n",
              " 'проспала',\n",
              " 'достаточно',\n",
              " 'долго',\n",
              " 'чтобы',\n",
              " 'сделать',\n",
              " 'неуместную',\n",
              " 'шутку',\n",
              " 'так',\n",
              " 'что',\n",
              " 'все',\n",
              " 'хорошо',\n",
              " 'говорится',\n",
              " 'на',\n",
              " 'ее',\n",
              " 'странице',\n",
              " 'в',\n",
              " 'Twitter',\n",
              " 'летняя',\n",
              " 'изобретательница',\n",
              " 'из',\n",
              " 'Стокгольма',\n",
              " 'прославилась',\n",
              " 'несколько',\n",
              " 'лет',\n",
              " 'назад',\n",
              " 'показывая',\n",
              " 'подписчикам',\n",
              " 'свои',\n",
              " 'бесполезные',\n",
              " 'изобретения',\n",
              " 'Например',\n",
              " 'Герц',\n",
              " 'демонстрировала',\n",
              " 'самодельный',\n",
              " 'будильник',\n",
              " 'к',\n",
              " 'которому',\n",
              " 'приделана',\n",
              " 'резиновая',\n",
              " 'рука',\n",
              " 'избивающая',\n",
              " 'спящего',\n",
              " 'человека',\n",
              " 'Кроме',\n",
              " 'того',\n",
              " 'девушка',\n",
              " 'создавала',\n",
              " 'автоматизированную',\n",
              " 'машину',\n",
              " 'для',\n",
              " 'наливания',\n",
              " 'молока',\n",
              " 'в',\n",
              " 'тарелку',\n",
              " 'с',\n",
              " 'хлопьями',\n",
              " 'Сама',\n",
              " 'Герц',\n",
              " 'называет',\n",
              " 'себя',\n",
              " 'королевой',\n",
              " 'дерьмовых',\n",
              " 'роботов',\n",
              " 'на',\n",
              " 'ее',\n",
              " 'канал',\n",
              " 'подписаны',\n",
              " 'более',\n",
              " 'миллиона',\n",
              " 'пользователей']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzIwCL0RD63O"
      },
      "source": [
        "Посчитаем частотность слов при помощи Counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRmVo8VfD63Q",
        "outputId": "a8069378-b5d1-463f-c4f8-2c43b98adab1"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "wdict = Counter(words) \n",
        "print(wdict)\n",
        "print({w:n for w,n in wdict.items() if n>1})"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'в': 5, 'Герц': 4, 'ее': 4, 'что': 3, 'из': 3, 'Симона': 2, 'Twitter': 2, 'девушка': 2, 'подписчикам': 2, 'опухоль': 2, 'с': 2, 'для': 2, 'очень': 2, 'врачи': 2, 'и': 2, 'все': 2, 'на': 2, 'Шведская': 1, 'YouTube-знаменитость': 1, 'Simone': 1, 'Giertz': 1, 'прославившаяся': 1, 'необычными': 1, 'изобретениями': 1, 'перенесла': 1, 'операцию': 1, 'Об': 1, 'этом': 1, 'рассказали': 1, 'представители': 1, 'Еще': 1, 'конце': 1, 'апреля': 1, 'сообщила': 1, 'о': 1, 'недуге': 1, 'Оказалось': 1, 'у': 1, 'нее': 1, 'мозге': 1, 'обнаружили': 1, 'доброкачественную': 1, 'размером': 1, 'мяч': 1, 'гольфа': 1, 'Блогер': 1, 'даже': 1, 'дала': 1, 'ей': 1, 'имя': 1, 'назвав': 1, 'новообразование': 1, 'Брайаном': 1, 'сильно': 1, 'переживала': 1, 'перед': 1, 'хирургическим': 1, 'вмешательством': 1, 'В': 1, 'одном': 1, 'предоперационных': 1, 'видео': 1, 'она': 1, 'говорила': 1, 'напугана': 1, 'Однако': 1, 'быстро': 1, 'удалили': 1, 'спасли': 1, 'девушке': 1, 'жизнь': 1, 'вышла': 1, 'хирургии': 1, 'довольны': 1, 'тем': 1, 'как': 1, 'прошло': 1, 'Она': 1, 'проспала': 1, 'достаточно': 1, 'долго': 1, 'чтобы': 1, 'сделать': 1, 'неуместную': 1, 'шутку': 1, 'так': 1, 'хорошо': 1, 'говорится': 1, 'странице': 1, 'летняя': 1, 'изобретательница': 1, 'Стокгольма': 1, 'прославилась': 1, 'несколько': 1, 'лет': 1, 'назад': 1, 'показывая': 1, 'свои': 1, 'бесполезные': 1, 'изобретения': 1, 'Например': 1, 'демонстрировала': 1, 'самодельный': 1, 'будильник': 1, 'к': 1, 'которому': 1, 'приделана': 1, 'резиновая': 1, 'рука': 1, 'избивающая': 1, 'спящего': 1, 'человека': 1, 'Кроме': 1, 'того': 1, 'создавала': 1, 'автоматизированную': 1, 'машину': 1, 'наливания': 1, 'молока': 1, 'тарелку': 1, 'хлопьями': 1, 'Сама': 1, 'называет': 1, 'себя': 1, 'королевой': 1, 'дерьмовых': 1, 'роботов': 1, 'канал': 1, 'подписаны': 1, 'более': 1, 'миллиона': 1, 'пользователей': 1})\n",
            "{'Симона': 2, 'Герц': 4, 'ее': 4, 'в': 5, 'Twitter': 2, 'девушка': 2, 'подписчикам': 2, 'что': 3, 'опухоль': 2, 'с': 2, 'для': 2, 'из': 3, 'очень': 2, 'врачи': 2, 'и': 2, 'все': 2, 'на': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7DSZQ4WD63S"
      },
      "source": [
        "Фактически, выше мы провели преобразование текста в вектор. Пространство вектора определено на словаре текста - количество измерений совпадает с количеством слов, каждому измерению сопоставлено какое-то слово и отложена его частота. Подобный подход называют мешком слов (Bag of Words, BoW), так как все слова перемешиваются, их порядок больше не соблюдается, а сами слова сваливаются в один \"мешок\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L4SI7ZjD63S"
      },
      "source": [
        "Обратите внимание на распределение частот в отдельных словах и парах. Такое распределение называется [распределением Ципфа](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%A6%D0%B8%D0%BF%D1%84%D0%B0) и является характерным практически для любого распределения частот слов и их комбинаций в текстах на любом естественном языке.\n",
        "\n",
        "Для расчета частот существует CountVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oWQuxDQD63U"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQiDINp5D63U",
        "outputId": "8487708a-3466-4ee6-b8af-396ac4571565"
      },
      "source": [
        "counter = CountVectorizer()\n",
        "# Просим посчитать частоты слов.\n",
        "res = counter.fit_transform([news.News.iloc[0]])\n",
        "# Разреженное представление счетчика.\n",
        "print(res[0,:10]) # По идентификатору можно получить частоту слова.\n",
        "# Можно получить индекс по слову, ...\n",
        "print('герц', counter.vocabulary_.get('герц'))\n",
        "# ... но не наоборот.\n",
        "print(counter.vocabulary_.get(18))\n",
        "print(counter.vocabulary_) # Словарь, который сопоставляет слову его идентификатор."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 3)\t2\n",
            "  (0, 6)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 9)\t1\n",
            "герц 18\n",
            "None\n",
            "{'шведская': 123, 'youtube': 4, 'знаменитость': 37, 'симона': 104, 'герц': 18, 'simone': 2, 'giertz': 1, 'прославившаяся': 89, 'необычными': 66, 'изобретениями': 42, 'перенесла': 81, 'операцию': 76, 'об': 70, 'этом': 125, 'рассказали': 94, 'ее': 33, 'представители': 87, 'twitter': 3, 'еще': 35, 'конце': 46, 'апреля': 6, 'девушка': 24, 'сообщила': 106, 'подписчикам': 83, 'недуге': 64, 'оказалось': 74, 'что': 121, 'нее': 65, 'мозге': 54, 'обнаружили': 71, 'доброкачественную': 29, 'опухоль': 77, 'размером': 93, 'мяч': 56, 'для': 28, 'гольфа': 21, 'блогер': 8, 'даже': 22, 'дала': 23, 'ей': 34, 'имя': 43, 'назвав': 59, 'новообразование': 69, 'брайаном': 10, 'сильно': 103, 'переживала': 80, 'перед': 79, 'хирургическим': 117, 'вмешательством': 14, 'одном': 73, 'из': 38, 'предоперационных': 86, 'видео': 13, 'она': 75, 'говорила': 19, 'очень': 78, 'напугана': 63, 'однако': 72, 'врачи': 15, 'быстро': 12, 'удалили': 115, 'спасли': 107, 'девушке': 25, 'жизнь': 36, 'вышла': 17, 'хирургии': 116, 'довольны': 30, 'тем': 113, 'как': 44, 'все': 16, 'прошло': 92, 'проспала': 91, 'достаточно': 32, 'долго': 31, 'чтобы': 122, 'сделать': 101, 'неуместную': 68, 'шутку': 124, 'так': 111, 'хорошо': 119, 'говорится': 20, 'на': 57, 'странице': 110, '27': 0, 'летняя': 51, 'изобретательница': 40, 'стокгольма': 109, 'прославилась': 90, 'несколько': 67, 'лет': 50, 'назад': 58, 'показывая': 84, 'свои': 100, 'бесполезные': 7, 'изобретения': 41, 'например': 62, 'демонстрировала': 26, 'самодельный': 99, 'будильник': 11, 'которому': 48, 'приделана': 88, 'резиновая': 95, 'рука': 97, 'избивающая': 39, 'спящего': 108, 'человека': 120, 'кроме': 49, 'того': 114, 'создавала': 105, 'автоматизированную': 5, 'машину': 52, 'наливания': 61, 'молока': 55, 'тарелку': 112, 'хлопьями': 118, 'сама': 98, 'называет': 60, 'себя': 102, 'королевой': 47, 'дерьмовых': 27, 'роботов': 96, 'канал': 45, 'подписаны': 82, 'более': 9, 'миллиона': 53, 'пользователей': 85}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQLR4uyD63V"
      },
      "source": [
        "На вход классу подается список текстов. Результатом работы является [разреженная матрица](https://docs.scipy.org/doc/scipy/reference/sparse.html) частот слов. Если мы возьмем большое количество текстов, то в каждом из них встречается не так много разных слов, но словарь всех текстов вместе будет огромен. Обработка текстов должна вестись в едином пространстве. Пусть это будет пространство словаря всех текстов. Получается, что для текста с маленьким словарем мы должны хранить большое число нулей. Для того, чтобы этого избежать, хранят, например, один раз номер строки, индексы ненулевых значений и сами значения, то есть чуть больше двух чисел на ненулевое значение. Если считать, что словарь одного текста - 100 слов, а словарь всех текстов - 100 000 слов, мы получаем экономию места в 500 раз.\n",
        "\n",
        "[Иллюстрация BOW](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-4/v-4/61)\n",
        "\n",
        "Обратите внимание, что все слова приведены к маленьким буквам, но про слова с дефисом этот класс ничего не знает.\n",
        "\n",
        "Чтобы исправить эту ситуацию можно передать собственное регулярное выражение для разделения на слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMm29TuJD63W",
        "outputId": "13c21247-697a-424a-ce2f-d6f469b158f2"
      },
      "source": [
        "counter=CountVectorizer(token_pattern=r'[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+')\n",
        "# Обратите внимание, передается список текстов.\n",
        "res=counter.fit_transform([news.News.iloc[0]])\n",
        "print(counter.vocabulary_) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'шведская': 127, 'youtube-знаменитость': 3, 'симона': 107, 'герц': 18, 'simone': 1, 'giertz': 0, 'прославившаяся': 91, 'необычными': 67, 'изобретениями': 42, 'перенесла': 83, 'операцию': 78, 'об': 72, 'этом': 129, 'рассказали': 96, 'ее': 33, 'представители': 89, 'в': 12, 'twitter': 2, 'еще': 35, 'конце': 47, 'апреля': 5, 'девушка': 24, 'сообщила': 109, 'подписчикам': 85, 'о': 71, 'недуге': 65, 'оказалось': 76, 'что': 125, 'у': 118, 'нее': 66, 'мозге': 55, 'обнаружили': 73, 'доброкачественную': 29, 'опухоль': 79, 'размером': 95, 'с': 100, 'мяч': 57, 'для': 28, 'гольфа': 21, 'блогер': 7, 'даже': 22, 'дала': 23, 'ей': 34, 'имя': 43, 'назвав': 60, 'новообразование': 70, 'брайаном': 9, 'сильно': 106, 'переживала': 82, 'перед': 81, 'хирургическим': 121, 'вмешательством': 14, 'одном': 75, 'из': 38, 'предоперационных': 88, 'видео': 13, 'она': 77, 'говорила': 19, 'очень': 80, 'напугана': 64, 'однако': 74, 'врачи': 15, 'быстро': 11, 'удалили': 119, 'и': 37, 'спасли': 110, 'девушке': 25, 'жизнь': 36, 'вышла': 17, 'хирургии': 120, 'довольны': 30, 'тем': 116, 'как': 45, 'все': 16, 'прошло': 94, 'проспала': 93, 'достаточно': 32, 'долго': 31, 'чтобы': 126, 'сделать': 104, 'неуместную': 69, 'шутку': 128, 'так': 114, 'хорошо': 123, 'говорится': 20, 'на': 58, 'странице': 113, 'летняя': 52, 'изобретательница': 40, 'стокгольма': 112, 'прославилась': 92, 'несколько': 68, 'лет': 51, 'назад': 59, 'показывая': 86, 'свои': 103, 'бесполезные': 6, 'изобретения': 41, 'например': 63, 'демонстрировала': 26, 'самодельный': 102, 'будильник': 10, 'к': 44, 'которому': 49, 'приделана': 90, 'резиновая': 97, 'рука': 99, 'избивающая': 39, 'спящего': 111, 'человека': 124, 'кроме': 50, 'того': 117, 'создавала': 108, 'автоматизированную': 4, 'машину': 53, 'наливания': 62, 'молока': 56, 'тарелку': 115, 'хлопьями': 122, 'сама': 101, 'называет': 61, 'себя': 105, 'королевой': 48, 'дерьмовых': 27, 'роботов': 98, 'канал': 46, 'подписаны': 84, 'более': 8, 'миллиона': 54, 'пользователей': 87}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC2KqYspD63X"
      },
      "source": [
        "Помимо этого, CountVectorizer умеет выделять n-граммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-wM7FOCD63Y",
        "outputId": "f0846716-5fd2-4a1c-cce6-130e9f76cd65"
      },
      "source": [
        "# ngram_range - это кортеж, который указывает длины последовательсностей, которые надо выделить. \n",
        "# Обязательны оба значения. Если хочется извлечь 4-граммы, надо передать (4,4).\n",
        "counter=CountVectorizer(ngram_range=(1,2), token_pattern=r'[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+')\n",
        "res=counter.fit_transform([news.News.iloc[0]])\n",
        "print(counter.vocabulary_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'шведская': 279, 'youtube-знаменитость': 7, 'симона': 236, 'герц': 43, 'simone': 2, 'giertz': 0, 'прославившаяся': 203, 'необычными': 152, 'изобретениями': 102, 'перенесла': 187, 'операцию': 175, 'об': 162, 'этом': 283, 'рассказали': 213, 'ее': 78, 'представители': 199, 'в': 25, 'twitter': 4, 'еще': 85, 'конце': 112, 'апреля': 11, 'девушка': 58, 'сообщила': 241, 'подписчикам': 191, 'о': 160, 'недуге': 148, 'оказалось': 170, 'что': 273, 'у': 259, 'нее': 150, 'мозге': 128, 'обнаружили': 164, 'доброкачественную': 70, 'опухоль': 177, 'размером': 211, 'с': 221, 'мяч': 132, 'для': 67, 'гольфа': 52, 'блогер': 15, 'даже': 54, 'дала': 56, 'ей': 83, 'имя': 104, 'назвав': 138, 'новообразование': 158, 'брайаном': 19, 'сильно': 234, 'переживала': 185, 'перед': 183, 'хирургическим': 265, 'вмешательством': 33, 'одном': 168, 'из': 92, 'предоперационных': 197, 'видео': 31, 'она': 172, 'говорила': 48, 'очень': 180, 'напугана': 146, 'однако': 166, 'врачи': 35, 'быстро': 23, 'удалили': 261, 'и': 89, 'спасли': 243, 'девушке': 61, 'жизнь': 87, 'вышла': 41, 'хирургии': 263, 'довольны': 72, 'тем': 255, 'как': 108, 'все': 38, 'прошло': 209, 'проспала': 207, 'достаточно': 76, 'долго': 74, 'чтобы': 277, 'сделать': 230, 'неуместную': 156, 'шутку': 281, 'так': 251, 'хорошо': 269, 'говорится': 50, 'на': 134, 'странице': 249, 'летняя': 122, 'изобретательница': 98, 'стокгольма': 247, 'прославилась': 205, 'несколько': 154, 'лет': 120, 'назад': 136, 'показывая': 194, 'свои': 228, 'бесполезные': 13, 'изобретения': 100, 'например': 144, 'демонстрировала': 63, 'самодельный': 226, 'будильник': 21, 'к': 106, 'которому': 116, 'приделана': 201, 'резиновая': 215, 'рука': 219, 'избивающая': 96, 'спящего': 245, 'человека': 271, 'кроме': 118, 'того': 257, 'создавала': 239, 'автоматизированную': 9, 'машину': 124, 'наливания': 142, 'молока': 130, 'тарелку': 253, 'хлопьями': 267, 'сама': 224, 'называет': 140, 'себя': 232, 'королевой': 114, 'дерьмовых': 65, 'роботов': 217, 'канал': 110, 'подписаны': 189, 'более': 17, 'миллиона': 126, 'пользователей': 196, 'шведская youtube-знаменитость': 280, 'youtube-знаменитость симона': 8, 'симона герц': 238, 'герц simone': 44, 'simone giertz': 3, 'giertz прославившаяся': 1, 'прославившаяся необычными': 204, 'необычными изобретениями': 153, 'изобретениями перенесла': 103, 'перенесла операцию': 188, 'операцию об': 176, 'об этом': 163, 'этом рассказали': 284, 'рассказали ее': 214, 'ее представители': 81, 'представители в': 200, 'в twitter': 26, 'twitter еще': 5, 'еще в': 86, 'в конце': 27, 'конце апреля': 113, 'апреля девушка': 12, 'девушка сообщила': 60, 'сообщила подписчикам': 242, 'подписчикам о': 192, 'о недуге': 161, 'недуге оказалось': 149, 'оказалось что': 171, 'что у': 276, 'у нее': 260, 'нее в': 151, 'в мозге': 28, 'мозге обнаружили': 129, 'обнаружили доброкачественную': 165, 'доброкачественную опухоль': 71, 'опухоль размером': 179, 'размером с': 212, 'с мяч': 222, 'мяч для': 133, 'для гольфа': 68, 'гольфа блогер': 53, 'блогер даже': 16, 'даже дала': 55, 'дала ей': 57, 'ей имя': 84, 'имя назвав': 105, 'назвав новообразование': 139, 'новообразование брайаном': 159, 'брайаном герц': 20, 'герц сильно': 47, 'сильно переживала': 235, 'переживала перед': 186, 'перед хирургическим': 184, 'хирургическим вмешательством': 266, 'вмешательством в': 34, 'в одном': 29, 'одном из': 169, 'из предоперационных': 93, 'предоперационных видео': 198, 'видео она': 32, 'она говорила': 173, 'говорила что': 49, 'что очень': 275, 'очень напугана': 182, 'напугана однако': 147, 'однако врачи': 167, 'врачи быстро': 36, 'быстро удалили': 24, 'удалили опухоль': 262, 'опухоль и': 178, 'и спасли': 91, 'спасли девушке': 244, 'девушке жизнь': 62, 'жизнь симона': 88, 'симона вышла': 237, 'вышла из': 42, 'из хирургии': 95, 'хирургии и': 264, 'и ее': 90, 'ее врачи': 79, 'врачи очень': 37, 'очень довольны': 181, 'довольны тем': 73, 'тем как': 256, 'как все': 109, 'все прошло': 39, 'прошло она': 210, 'она проспала': 174, 'проспала достаточно': 208, 'достаточно долго': 77, 'долго чтобы': 75, 'чтобы сделать': 278, 'сделать неуместную': 231, 'неуместную шутку': 157, 'шутку так': 282, 'так что': 252, 'что все': 274, 'все хорошо': 40, 'хорошо говорится': 270, 'говорится на': 51, 'на ее': 135, 'ее странице': 82, 'странице в': 250, 'twitter летняя': 6, 'летняя изобретательница': 123, 'изобретательница из': 99, 'из стокгольма': 94, 'стокгольма прославилась': 248, 'прославилась несколько': 206, 'несколько лет': 155, 'лет назад': 121, 'назад показывая': 137, 'показывая подписчикам': 195, 'подписчикам свои': 193, 'свои бесполезные': 229, 'бесполезные изобретения': 14, 'изобретения например': 101, 'например герц': 145, 'герц демонстрировала': 45, 'демонстрировала самодельный': 64, 'самодельный будильник': 227, 'будильник к': 22, 'к которому': 107, 'которому приделана': 117, 'приделана резиновая': 202, 'резиновая рука': 216, 'рука избивающая': 220, 'избивающая спящего': 97, 'спящего человека': 246, 'человека кроме': 272, 'кроме того': 119, 'того девушка': 258, 'девушка создавала': 59, 'создавала автоматизированную': 240, 'автоматизированную машину': 10, 'машину для': 125, 'для наливания': 69, 'наливания молока': 143, 'молока в': 131, 'в тарелку': 30, 'тарелку с': 254, 'с хлопьями': 223, 'хлопьями сама': 268, 'сама герц': 225, 'герц называет': 46, 'называет себя': 141, 'себя королевой': 233, 'королевой дерьмовых': 115, 'дерьмовых роботов': 66, 'роботов на': 218, 'ее канал': 80, 'канал подписаны': 111, 'подписаны более': 190, 'более миллиона': 18, 'миллиона пользователей': 127}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI_x3JU_D63Z"
      },
      "source": [
        "Выделим пять самых частотных слов или словосочетаний из текста новости. Воспользуемся тем фактом, что CountVectorizer умеет работать со списками новостей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0CgoVFMD63a"
      },
      "source": [
        "def getMostFrequentWords(text):\n",
        "    counter=CountVectorizer(ngram_range=(1,2), token_pattern=r'[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+')\n",
        "    res=counter.fit_transform([text])\n",
        "    frq = sorted([(i, int(res[0][0,i])) for i in range(res[0].shape[1])], key = lambda x: x[1], reverse = True)[:5]\n",
        "    vocab = {w:res[0,i] for w,i in counter.vocabulary_.items()}\n",
        "    return [[w for w, n in counter.vocabulary_.items() if n==i][0] for i, f in frq], vocab\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egInrHJ0D63a",
        "outputId": "4a566ad1-d46d-482f-9afc-6126806aa68e"
      },
      "source": [
        "   \n",
        "freqwords = []\n",
        "vocabs = []\n",
        "for i in tqdm(range(news.shape[0])):\n",
        "    fw, v = getMostFrequentWords(news.News.iloc[i])\n",
        "    freqwords.append(fw)\n",
        "    vocabs.append(v)\n",
        "    \n",
        "news['Vocabular'] = vocabs\n",
        "news['Freq Words'] = freqwords"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7816/7816 [05:09<00:00, 25.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "-IN-3miWD63c",
        "outputId": "9954f6e0-9ffb-4f9c-d78d-8f4186f17ec2"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "      <th>Vocabular</th>\n",
              "      <th>Freq Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«Королева дерьмовых роботов» перенесла операци...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Шведская YouTube-знаменитость Симона Герц (Sim...</td>\n",
              "      <td>{'шведская': 1, 'youtube-знаменитость': 1, 'си...</td>\n",
              "      <td>[в, герц, ее, из, что]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>У Су-57 нашли «тайные ноу-хау»</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Перспективный российский многофункциональный и...</td>\n",
              "      <td>{'перспективный': 1, 'российский': 1, 'многофу...</td>\n",
              "      <td>[су, в, не, на, ни]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Знарок лишился еще одного поста</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Санкт-петербургский клуб Континентальной хокке...</td>\n",
              "      <td>{'санкт-петербургский': 1, 'клуб': 2, 'контине...</td>\n",
              "      <td>[и, в, знарок, ска, тренера]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>СК заинтересовался задержанием аутиста московс...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Следственный комитет по Москве начал проверку ...</td>\n",
              "      <td>{'следственный': 1, 'комитет': 1, 'по': 3, 'мо...</td>\n",
              "      <td>[в, его, и, к, по]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Киевляне перекрыли проспект из-за сбитого поли...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Более 100 жителей Киева перекрыли проспект Гри...</td>\n",
              "      <td>{'более': 1, 'жителей': 1, 'киева': 2, 'перекр...</td>\n",
              "      <td>[в, на, автомобиль, в котором, дтп]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Header  ...                           Freq Words\n",
              "0  «Королева дерьмовых роботов» перенесла операци...  ...               [в, герц, ее, из, что]\n",
              "1                     У Су-57 нашли «тайные ноу-хау»  ...                  [су, в, не, на, ни]\n",
              "2                    Знарок лишился еще одного поста  ...         [и, в, знарок, ска, тренера]\n",
              "3  СК заинтересовался задержанием аутиста московс...  ...                   [в, его, и, к, по]\n",
              "4  Киевляне перекрыли проспект из-за сбитого поли...  ...  [в, на, автомобиль, в котором, дтп]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha8VepyXD63d"
      },
      "source": [
        "Подключим морфологический анализ, чтобы избавиться от стоп-слов и привести формы слова к лемме. Начнем с Pymorphy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOHVwMPndxip"
      },
      "source": [
        "import pymorphy2"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eICPz-QPD63e",
        "outputId": "5725442c-3723-4f5b-a849-e23b4cdf70cd"
      },
      "source": [
        "morph=pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
        "wordform=morph.parse('стекло')  # Проведем анализ слова \"стекло\"...\n",
        "print(wordform)                 # ... и посмотрим на результат."
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),)), Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),)), Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae90sHx0ejHT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfvikzyXD63f",
        "outputId": "a5d6d239-5b47-4621-b56c-37cc6a1f9d4c"
      },
      "source": [
        "wordform=morph.parse('ёрничали') \n",
        "wordform"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='ёрничали', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='ёрничать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'ёрничали', 15, 10),))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCQM3bI9D63g",
        "outputId": "8a8586bc-9a1e-4d53-cbf4-4208c9d63be2"
      },
      "source": [
        "wordform[0].score"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIQhpjjRD63j",
        "outputId": "442d33c8-679c-4e85-e0a6-e72b762d3b2d"
      },
      "source": [
        "wordform[0].tag"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpencorporaTag('VERB,impf,intr plur,past,indc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB3RfShlD63n"
      },
      "source": [
        "Самым простым видом борьбы с омонимией является выбор нулевого элемента из списка, возвращенного морфологическим анализом. Такой подход дает около 90% точности при выборе начальной формы и до 80% если мы обращаем внимание на грамматические параметры.<br><br>\n",
        "Вместо Pymorphy можно использовать PyMystem. Его плюсом является тот факт, что он сам снимает омонимию. Используя функцию lemmatize можно получить набор начальных форм слов. Используя функцию analyze можно получить полную информацию о словах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I9OzrM9D63o"
      },
      "source": [
        "import pymystem3 # Еще один морфологический анализатор. При первом запуске грузит словари из Сети."
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqo00UmyycQL"
      },
      "source": [
        "mystem=pymystem3.Mystem()\r\n",
        "print(mystem.lemmatize('эти типы стали есть в цеху.'))\r\n",
        "print(mystem.analyze('эти типы стали есть в цеху.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v50kvNy7D63r",
        "outputId": "e166f911-eae0-446c-8c3b-d5a8eb6e5fb5"
      },
      "source": [
        "my_res=mystem.analyze('эти типы стали есть в цеху.')\n",
        "if 'analysis' in my_res[0].keys():\n",
        "    print(my_res[0]['analysis'][0]['gr'].split(\"=\")[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "APRO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uch8rUhiD63s"
      },
      "source": [
        "Функция `lemmatize` делит текст на слова и знаки препинания, а затем возвращает для них только начальную форму.\n",
        "\n",
        "Функция `analyze` возвращает не только начальную форму, но и всю информацию о слове, как это делал перед этим Pymorphy. \n",
        "\n",
        "Основным отличием является то, что Mystem снимает омонимию. Как видно из примера, делает он это не всегда корректно, но нам не придется думать о том, какой вариант разбора следует взять.\n",
        "\n",
        "Еще одна библиотека - NLTK. По сравнению с двумя предыдущими библиотеками она обладает более широкой функциональностью и изначально писалась для работы с разными языками."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyiR9Wz7D63s"
      },
      "source": [
        "import nltk # Иностранный морфологический анализатор"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYMYSp5CD63s"
      },
      "source": [
        "Перед началом использования необходимо загрузить необходимые библиотеки или корпуса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvXw4ExOD63u",
        "outputId": "ea8b48e6-da08-410e-989c-cebf2a5aef2e"
      },
      "source": [
        "nltk.download() # По дороге будут появляться поле ввода. Грузит всё из Сети."
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hotc2UlGD63v"
      },
      "source": [
        "Можно сразу скачать нужный пакет, если вы знаете, как он назыввается."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvZ-BqcVD63v",
        "outputId": "fc48d837-98cb-4530-a9ea-499af50f24ec"
      },
      "source": [
        "nltk.download(['averaged_perceptron_tagger_ru', 'stopwords'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmKjgqAD63v"
      },
      "source": [
        "Функция `word_tokenize` возвращает начальные формы слов. \n",
        "\n",
        "Функция `pos_tag` возвращает список начальных форм и их частей речи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4TWQ_w1ib8M",
        "outputId": "3140b69d-03b3-4c29-b336-22426dc60583"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdKIL63VD63w",
        "outputId": "6f8a5e39-29e9-4e05-a4f2-a3c03a5c4b58"
      },
      "source": [
        "tokens = nltk.word_tokenize('Эти типы стали есть в цеху') # Токенизация.\n",
        "bi_tokens = list(nltk.bigrams(tokens))\n",
        "tokens, bi_tokens"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Эти', 'типы', 'стали', 'есть', 'в', 'цеху'],\n",
              " [('Эти', 'типы'),\n",
              "  ('типы', 'стали'),\n",
              "  ('стали', 'есть'),\n",
              "  ('есть', 'в'),\n",
              "  ('в', 'цеху')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oh2AsigD63y",
        "outputId": "5c79969e-d178-4ce6-fdb0-a58369cb2878"
      },
      "source": [
        "pos = nltk.pos_tag(tokens) # Частеречная разметка.\n",
        "bi_pos = list(nltk.bigrams(pos))\n",
        "pos, bi_pos\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('Эти', 'JJ'),\n",
              "  ('типы', 'NNP'),\n",
              "  ('стали', 'NNP'),\n",
              "  ('есть', 'NNP'),\n",
              "  ('в', 'NNP'),\n",
              "  ('цеху', 'NN')],\n",
              " [(('Эти', 'JJ'), ('типы', 'NNP')),\n",
              "  (('типы', 'NNP'), ('стали', 'NNP')),\n",
              "  (('стали', 'NNP'), ('есть', 'NNP')),\n",
              "  (('есть', 'NNP'), ('в', 'NNP')),\n",
              "  (('в', 'NNP'), ('цеху', 'NN'))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtmwlHPID63z"
      },
      "source": [
        "У NLTK заведен список стоп-слов, которые лучше фильтровать при анализе текстов. Но их не очень много."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwLDiP4ED63z",
        "outputId": "26fb1ddf-9895-471b-81ed-ab88ddab16a3"
      },
      "source": [
        "# Оставим только те слова, которых нет в списке стоп-слов.\n",
        "filtered_words = [token for token in tokens if token not in nltk.corpus.stopwords.words('russian')]\n",
        "print('всего русских стоп-слов', len(nltk.corpus.stopwords.words('russian')))\n",
        "filtered_words"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "всего русских стоп-слов 151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Эти', 'типы', 'стали', 'цеху']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XpiU5qRD63z"
      },
      "source": [
        "Ту же самую задачу в других библиотеках можно решить при помощи фильтра частей речи. Можно считать, что значимыми являются лишь существительные, прилагательные, глаголы, причастия и деепричастия. Ниже приведены названия частей речи для разных библиотек.\n",
        "<table>\n",
        "<tr><th>Часть речи</th><th>Pymorphy</th><th>Mystem</th><th>NLTK</th></tr>\n",
        "<tr><td>Существительное</td><td>NOUN</td><td>S</td><td>NN</td></tr>\n",
        "<tr><td>Прилагательное</td><td>ADJF, ADJS</td><td>A</td><td>NNP</td></tr>\n",
        "<tr><td>Глагол</td><td>VERB</td><td>V</td><td>JJ</td></tr>\n",
        "<tr><td>Причастие</td><td>PRTF, PRTS</td><td>V</td><td>NNP</td></tr>\n",
        "<tr><td>Деепричастие</td><td>GRND</td><td>V</td><td>NNP</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBtgsGunD630"
      },
      "source": [
        "Приведем все слова текста к начальным формам при помощи разных библиотек. Прибавим при этом к словам части речи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUlOERR5D632",
        "outputId": "b72c6259-70f7-474a-8b0a-af0f6c9e7936"
      },
      "source": [
        "# Pymorphy\n",
        "def normalizePymorphy(text):\n",
        "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
        "    words = []\n",
        "    for t in tokens:\n",
        "        pv = morph.parse(t)\n",
        "        words.append(pv[0].normal_form + '_' + str(pv[0].tag.POS)) # Берем наиболее вероятную форму.\n",
        "    return words    \n",
        "        \n",
        "# Обратите внимание, что про иностранные слова словарь ничего не знает.\n",
        "normalizePymorphy(news.News.iloc[0])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['шведский_ADJF',\n",
              " 'youtube-знаменитость_NOUN',\n",
              " 'симона_NOUN',\n",
              " 'герц_NOUN',\n",
              " 'simone_None',\n",
              " 'giertz_None',\n",
              " 'прославиться_PRTF',\n",
              " 'необычный_ADJF',\n",
              " 'изобретение_NOUN',\n",
              " 'перенести_VERB',\n",
              " 'операция_NOUN',\n",
              " 'о_PREP',\n",
              " 'это_NPRO',\n",
              " 'рассказать_VERB',\n",
              " 'она_NPRO',\n",
              " 'представитель_NOUN',\n",
              " 'в_PREP',\n",
              " 'twitter_None',\n",
              " 'ещё_ADVB',\n",
              " 'в_PREP',\n",
              " 'конец_NOUN',\n",
              " 'апрель_NOUN',\n",
              " 'девушка_NOUN',\n",
              " 'сообщить_VERB',\n",
              " 'подписчик_NOUN',\n",
              " 'о_PREP',\n",
              " 'недуг_NOUN',\n",
              " 'оказаться_VERB',\n",
              " 'что_CONJ',\n",
              " 'у_PREP',\n",
              " 'она_NPRO',\n",
              " 'в_PREP',\n",
              " 'мозг_NOUN',\n",
              " 'обнаружить_VERB',\n",
              " 'доброкачественный_ADJF',\n",
              " 'опухоль_NOUN',\n",
              " 'размер_NOUN',\n",
              " 'с_PREP',\n",
              " 'мяч_NOUN',\n",
              " 'для_PREP',\n",
              " 'гольф_NOUN',\n",
              " 'блогер_NOUN',\n",
              " 'даже_PRCL',\n",
              " 'дать_VERB',\n",
              " 'она_NPRO',\n",
              " 'имя_NOUN',\n",
              " 'назвать_GRND',\n",
              " 'новообразование_NOUN',\n",
              " 'брайан_NOUN',\n",
              " 'герц_NOUN',\n",
              " 'сильно_ADVB',\n",
              " 'переживать_VERB',\n",
              " 'перед_PREP',\n",
              " 'хирургический_ADJF',\n",
              " 'вмешательство_NOUN',\n",
              " 'в_PREP',\n",
              " 'один_ADJF',\n",
              " 'из_PREP',\n",
              " 'предоперационный_ADJF',\n",
              " 'видео_NOUN',\n",
              " 'она_NPRO',\n",
              " 'говорить_VERB',\n",
              " 'что_CONJ',\n",
              " 'очень_ADVB',\n",
              " 'напугать_PRTS',\n",
              " 'однако_CONJ',\n",
              " 'врач_NOUN',\n",
              " 'быстро_ADVB',\n",
              " 'удалить_VERB',\n",
              " 'опухоль_NOUN',\n",
              " 'и_CONJ',\n",
              " 'спасти_VERB',\n",
              " 'девушка_NOUN',\n",
              " 'жизнь_NOUN',\n",
              " 'симона_NOUN',\n",
              " 'выйти_VERB',\n",
              " 'из_PREP',\n",
              " 'хирургия_NOUN',\n",
              " 'и_CONJ',\n",
              " 'она_NPRO',\n",
              " 'врач_NOUN',\n",
              " 'очень_ADVB',\n",
              " 'довольный_ADJS',\n",
              " 'тем_CONJ',\n",
              " 'как_CONJ',\n",
              " 'всё_PRCL',\n",
              " 'пройти_VERB',\n",
              " 'она_NPRO',\n",
              " 'проспать_VERB',\n",
              " 'достаточно_ADVB',\n",
              " 'долго_ADVB',\n",
              " 'чтобы_CONJ',\n",
              " 'сделать_INFN',\n",
              " 'неуместный_ADJF',\n",
              " 'шутка_NOUN',\n",
              " 'так_CONJ',\n",
              " 'что_CONJ',\n",
              " 'всё_PRCL',\n",
              " 'хорошо_ADVB',\n",
              " 'говориться_VERB',\n",
              " 'на_PREP',\n",
              " 'она_NPRO',\n",
              " 'страница_NOUN',\n",
              " 'в_PREP',\n",
              " 'twitter_None',\n",
              " 'летний_ADJF',\n",
              " 'изобретательница_NOUN',\n",
              " 'из_PREP',\n",
              " 'стокгольм_NOUN',\n",
              " 'прославиться_VERB',\n",
              " 'несколько_ADVB',\n",
              " 'год_NOUN',\n",
              " 'назад_ADVB',\n",
              " 'показывать_GRND',\n",
              " 'подписчик_NOUN',\n",
              " 'свой_ADJF',\n",
              " 'бесполезный_ADJF',\n",
              " 'изобретение_NOUN',\n",
              " 'например_CONJ',\n",
              " 'герц_NOUN',\n",
              " 'демонстрировать_VERB',\n",
              " 'самодельный_ADJF',\n",
              " 'будильник_NOUN',\n",
              " 'к_PREP',\n",
              " 'который_ADJF',\n",
              " 'приделать_PRTS',\n",
              " 'резиновый_ADJF',\n",
              " 'рука_NOUN',\n",
              " 'избивать_PRTF',\n",
              " 'спать_PRTF',\n",
              " 'человек_NOUN',\n",
              " 'кроме_PREP',\n",
              " 'тот_ADJF',\n",
              " 'девушка_NOUN',\n",
              " 'создавать_VERB',\n",
              " 'автоматизированный_ADJF',\n",
              " 'машина_NOUN',\n",
              " 'для_PREP',\n",
              " 'наливание_NOUN',\n",
              " 'молоко_NOUN',\n",
              " 'в_PREP',\n",
              " 'тарелка_NOUN',\n",
              " 'с_PREP',\n",
              " 'хлопья_NOUN',\n",
              " 'сам_ADJF',\n",
              " 'герц_NOUN',\n",
              " 'называть_VERB',\n",
              " 'себя_NPRO',\n",
              " 'королева_NOUN',\n",
              " 'дерьмовый_ADJF',\n",
              " 'робот_NOUN',\n",
              " 'на_PREP',\n",
              " 'она_NPRO',\n",
              " 'канал_NOUN',\n",
              " 'подписать_PRTS',\n",
              " 'более_ADVB',\n",
              " 'миллион_NOUN',\n",
              " 'пользователь_NOUN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfwaQQttD633",
        "outputId": "f41bbf1f-b690-465f-bfee-7dae55257d9b"
      },
      "source": [
        "# PyMystem\n",
        "def normalizePymystem(text):\n",
        "    tokens = mystem.analyze(text)\n",
        "    words = []\n",
        "    for t in tokens:\n",
        "        if 'analysis' in t.keys():\n",
        "            if t['analysis'] != []:\n",
        "                words.append(t['analysis'][0]['lex']+'_'+t['analysis'][0]['gr'][0])\n",
        "            else:\n",
        "                words.append(t['text']+'_'+'U')\n",
        "    return words    \n",
        "        \n",
        "# Не все считают, что причастие всегда выступает в роли глагола, но иногда так значительно проще.\n",
        "normalizePymystem(news.News.iloc[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['шведский_A',\n",
              " 'YouTube_U',\n",
              " 'знаменитость_S',\n",
              " 'симона_S',\n",
              " 'герц_S',\n",
              " 'Simone_U',\n",
              " 'Giertz_U',\n",
              " 'прославляться_V',\n",
              " 'необычный_A',\n",
              " 'изобретение_S',\n",
              " 'перенести_V',\n",
              " 'операция_S',\n",
              " 'об_P',\n",
              " 'это_S',\n",
              " 'рассказывать_V',\n",
              " 'она_S',\n",
              " 'представитель_S',\n",
              " 'в_P',\n",
              " 'Twitter_U',\n",
              " 'еще_A',\n",
              " 'в_P',\n",
              " 'конец_S',\n",
              " 'апрель_S',\n",
              " 'девушка_S',\n",
              " 'сообщать_V',\n",
              " 'подписчик_S',\n",
              " 'о_P',\n",
              " 'недуг_S',\n",
              " 'оказываться_V',\n",
              " 'что_C',\n",
              " 'у_P',\n",
              " 'она_S',\n",
              " 'в_P',\n",
              " 'мозг_S',\n",
              " 'обнаруживать_V',\n",
              " 'доброкачественный_A',\n",
              " 'опухоль_S',\n",
              " 'размер_S',\n",
              " 'с_P',\n",
              " 'мяч_S',\n",
              " 'для_P',\n",
              " 'гольф_S',\n",
              " 'блогер_S',\n",
              " 'даже_C',\n",
              " 'давать_V',\n",
              " 'она_S',\n",
              " 'имя_S',\n",
              " 'называть_V',\n",
              " 'новообразование_S',\n",
              " 'брайан_S',\n",
              " 'герц_S',\n",
              " 'сильно_A',\n",
              " 'переживать_V',\n",
              " 'перед_P',\n",
              " 'хирургический_A',\n",
              " 'вмешательство_S',\n",
              " 'в_P',\n",
              " 'один_A',\n",
              " 'из_P',\n",
              " 'предоперационный_A',\n",
              " 'видео_S',\n",
              " 'она_S',\n",
              " 'говорить_V',\n",
              " 'что_C',\n",
              " 'очень_A',\n",
              " 'напугать_V',\n",
              " 'однако_C',\n",
              " 'врач_S',\n",
              " 'быстро_A',\n",
              " 'удалять_V',\n",
              " 'опухоль_S',\n",
              " 'и_C',\n",
              " 'спасать_V',\n",
              " 'девушка_S',\n",
              " 'жизнь_S',\n",
              " 'симона_S',\n",
              " 'выходить_V',\n",
              " 'из_P',\n",
              " 'хирургия_S',\n",
              " 'и_C',\n",
              " 'ее_A',\n",
              " 'врач_S',\n",
              " 'очень_A',\n",
              " 'довольный_A',\n",
              " 'то_S',\n",
              " 'как_C',\n",
              " 'все_S',\n",
              " 'проходить_V',\n",
              " 'она_S',\n",
              " 'просыпать_V',\n",
              " 'достаточно_A',\n",
              " 'долго_A',\n",
              " 'чтобы_C',\n",
              " 'сделать_V',\n",
              " 'неуместный_A',\n",
              " 'шутка_S',\n",
              " 'так_A',\n",
              " 'что_C',\n",
              " 'все_S',\n",
              " 'хорошо_A',\n",
              " 'говориться_V',\n",
              " 'на_P',\n",
              " 'ее_A',\n",
              " 'страница_S',\n",
              " 'в_P',\n",
              " 'Twitter_U',\n",
              " 'летний_A',\n",
              " 'изобретательница_S',\n",
              " 'из_P',\n",
              " 'стокгольм_S',\n",
              " 'прославляться_V',\n",
              " 'несколько_N',\n",
              " 'год_S',\n",
              " 'назад_A',\n",
              " 'показывать_V',\n",
              " 'подписчик_S',\n",
              " 'свой_A',\n",
              " 'бесполезный_A',\n",
              " 'изобретение_S',\n",
              " 'например_A',\n",
              " 'герц_S',\n",
              " 'демонстрировать_V',\n",
              " 'самодельный_A',\n",
              " 'будильник_S',\n",
              " 'к_P',\n",
              " 'который_A',\n",
              " 'приделывать_V',\n",
              " 'резиновый_A',\n",
              " 'рука_S',\n",
              " 'избивать_V',\n",
              " 'спать_V',\n",
              " 'человек_S',\n",
              " 'кроме_P',\n",
              " 'то_S',\n",
              " 'девушка_S',\n",
              " 'создавать_V',\n",
              " 'автоматизировать_V',\n",
              " 'машина_S',\n",
              " 'для_P',\n",
              " 'наливание_S',\n",
              " 'молоко_S',\n",
              " 'в_P',\n",
              " 'тарелка_S',\n",
              " 'с_P',\n",
              " 'хлопья_S',\n",
              " 'сам_A',\n",
              " 'герц_S',\n",
              " 'называть_V',\n",
              " 'себя_S',\n",
              " 'королева_S',\n",
              " 'дерьмовый_A',\n",
              " 'робот_S',\n",
              " 'на_P',\n",
              " 'она_S',\n",
              " 'канал_S',\n",
              " 'подписывать_V',\n",
              " 'более_A',\n",
              " 'миллион_S',\n",
              " 'пользователь_S']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4uBnIeKD636",
        "outputId": "4c110a43-f593-458e-e1b7-f01a927d26ef"
      },
      "source": [
        "# NLTK\n",
        "def normalizeNLTK(text):\n",
        "    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    words = []\n",
        "    for t in tokens:\n",
        "        if t[0] != t[1]:\n",
        "            words.append(t[0]+'_'+t[1])\n",
        "    return words    \n",
        "        \n",
        "# А вот здесь с частеречной разметкой всё плохо, а параметров нет вовсе.\n",
        "normalizeNLTK(news.News.iloc[0])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Шведская_JJ',\n",
              " 'YouTube-знаменитость_NNP',\n",
              " 'Симона_NNP',\n",
              " 'Герц_NNP',\n",
              " 'Simone_NNP',\n",
              " 'Giertz_NNP',\n",
              " 'прославившаяся_JJ',\n",
              " 'необычными_NNP',\n",
              " 'изобретениями_NNP',\n",
              " 'перенесла_NNP',\n",
              " 'операцию_NNP',\n",
              " 'Об_VB',\n",
              " 'этом_JJ',\n",
              " 'рассказали_NNP',\n",
              " 'ее_NNP',\n",
              " 'представители_NNP',\n",
              " 'в_NNP',\n",
              " 'Twitter_NNP',\n",
              " 'Еще_VB',\n",
              " 'в_JJ',\n",
              " 'конце_NNP',\n",
              " 'апреля_NNP',\n",
              " 'девушка_NNP',\n",
              " 'сообщила_NNP',\n",
              " 'подписчикам_NNP',\n",
              " 'о_NNP',\n",
              " 'недуге_NNP',\n",
              " 'Оказалось_NN',\n",
              " 'что_NNP',\n",
              " 'у_NNP',\n",
              " 'нее_NNP',\n",
              " 'в_NNP',\n",
              " 'мозге_NNP',\n",
              " 'обнаружили_NNP',\n",
              " 'доброкачественную_NNP',\n",
              " 'опухоль_NNP',\n",
              " 'размером_NNP',\n",
              " 'с_NNP',\n",
              " 'мяч_NNP',\n",
              " 'для_NNP',\n",
              " 'гольфа_NNP',\n",
              " 'Блогер_VB',\n",
              " 'даже_JJ',\n",
              " 'дала_NNP',\n",
              " 'ей_NNP',\n",
              " 'имя_NNP',\n",
              " 'назвав_NNP',\n",
              " 'новообразование_NNP',\n",
              " 'Брайаном_NNP',\n",
              " 'Герц_VB',\n",
              " 'сильно_JJ',\n",
              " 'переживала_NNP',\n",
              " 'перед_NNP',\n",
              " 'хирургическим_NNP',\n",
              " 'вмешательством_NNP',\n",
              " 'В_VB',\n",
              " 'одном_JJ',\n",
              " 'из_NNP',\n",
              " 'предоперационных_NNP',\n",
              " 'видео_NNP',\n",
              " 'она_NNP',\n",
              " 'говорила_NNP',\n",
              " 'что_NNP',\n",
              " 'очень_NNP',\n",
              " 'напугана_NNP',\n",
              " 'Однако_VB',\n",
              " 'врачи_JJ',\n",
              " 'быстро_NNP',\n",
              " 'удалили_NNP',\n",
              " 'опухоль_NNP',\n",
              " 'и_NNP',\n",
              " 'спасли_NNP',\n",
              " 'девушке_NNP',\n",
              " 'жизнь_NNP',\n",
              " '«_VB',\n",
              " 'Симона_JJ',\n",
              " 'вышла_NNP',\n",
              " 'из_NNP',\n",
              " 'хирургии_NNP',\n",
              " 'и_NNP',\n",
              " 'ее_NNP',\n",
              " 'врачи_NNP',\n",
              " 'очень_NNP',\n",
              " 'довольны_NNP',\n",
              " 'тем_NNP',\n",
              " 'как_NNP',\n",
              " 'все_NNP',\n",
              " 'прошло_NNP',\n",
              " 'Она_VB',\n",
              " 'проспала_JJ',\n",
              " 'достаточно_NNP',\n",
              " 'долго_NNP',\n",
              " 'чтобы_NNP',\n",
              " 'сделать_NNP',\n",
              " 'неуместную_NNP',\n",
              " 'шутку_NNP',\n",
              " 'так_NNP',\n",
              " 'что_NNP',\n",
              " 'все_NNP',\n",
              " 'хорошо_NNP',\n",
              " '»_NNP',\n",
              " '—_NNP',\n",
              " 'говорится_NNP',\n",
              " 'на_NNP',\n",
              " 'ее_NNP',\n",
              " 'странице_NNP',\n",
              " 'в_NNP',\n",
              " 'Twitter_NNP',\n",
              " '27-летняя_JJ',\n",
              " 'изобретательница_JJ',\n",
              " 'из_NN',\n",
              " 'Стокгольма_NNP',\n",
              " 'прославилась_NNP',\n",
              " 'несколько_NNP',\n",
              " 'лет_NNP',\n",
              " 'назад_NNP',\n",
              " 'показывая_NNP',\n",
              " 'подписчикам_NNP',\n",
              " 'свои_NNP',\n",
              " 'бесполезные_NNP',\n",
              " 'изобретения_NNP',\n",
              " 'Например_NN',\n",
              " 'Герц_NNP',\n",
              " 'демонстрировала_NNP',\n",
              " 'самодельный_NNP',\n",
              " 'будильник_NNP',\n",
              " 'к_NNP',\n",
              " 'которому_NNP',\n",
              " 'приделана_NNP',\n",
              " 'резиновая_NNP',\n",
              " 'рука_NNP',\n",
              " 'избивающая_NNP',\n",
              " 'спящего_NNP',\n",
              " 'человека_NNP',\n",
              " 'Кроме_NN',\n",
              " 'того_NN',\n",
              " 'девушка_NNP',\n",
              " 'создавала_NNP',\n",
              " 'автоматизированную_NNP',\n",
              " 'машину_NNP',\n",
              " 'для_NNP',\n",
              " 'наливания_NNP',\n",
              " 'молока_NNP',\n",
              " 'в_NNP',\n",
              " 'тарелку_NNP',\n",
              " 'с_NNP',\n",
              " 'хлопьями_NNP',\n",
              " 'Сама_VB',\n",
              " 'Герц_JJ',\n",
              " 'называет_NNP',\n",
              " 'себя_NNP',\n",
              " '«_NNP',\n",
              " 'королевой_NNP',\n",
              " 'дерьмовых_NNP',\n",
              " 'роботов_NNP',\n",
              " '»_NNP',\n",
              " 'на_NNP',\n",
              " 'ее_NNP',\n",
              " 'канал_NNP',\n",
              " 'подписаны_NNP',\n",
              " 'более_NNP',\n",
              " 'миллиона_NNP',\n",
              " 'пользователей_NNP']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnkFdZhXD637"
      },
      "source": [
        "imp_POS = ['ADJF', 'ADJS', 'NOUN', 'VERB', 'PRTF', 'PRTS', 'GRND']\n",
        "\n",
        "def getMostFrequentWordsFiltered(text):\n",
        "    \n",
        "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
        "    words = []\n",
        "    for t in tokens:\n",
        "        pv = morph.parse(t)\n",
        "        if pv[0].tag.POS in imp_POS and pv[0].normal_form != 'быть':\n",
        "            words.append(pv[0].normal_form)\n",
        "    text = ' '.join(words)\n",
        "    \n",
        "    counter=CountVectorizer(ngram_range=(1,2), token_pattern=r'[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+')\n",
        "    res=counter.fit_transform([text])\n",
        "    frq = sorted([(i, int(res[0][0,i])) for i in range(res[0].shape[1])], key = lambda x: x[1], reverse = True)[:5]\n",
        "    vocab = {w:res[0,i] for w,i in counter.vocabulary_.items()}\n",
        "    return [[w for w, n in counter.vocabulary_.items() if n==i][0] for i, f in frq], vocab"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqq1_6GRD637"
      },
      "source": [
        "Теперь посмотрим какие слова оказываются наиболее частотными, если выкинуть \"лишние\".\n",
        "\n",
        "Не забывайте, что в некоторых задачах эти слова, а также формы слова, могут оказаться нужными и важными."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NZJL1LQD638",
        "outputId": "209f4ba7-719c-401d-b823-8653e68a3013"
      },
      "source": [
        "freqwords = []\n",
        "vocabs = []\n",
        "for i in tqdm(range(news.shape[0])):\n",
        "    fw, v = getMostFrequentWordsFiltered(news.News.iloc[i])\n",
        "    freqwords.append(fw)\n",
        "    vocabs.append(v)\n",
        "    \n",
        "news['Vocabular'] = vocabs\n",
        "news['Freq Words'] = freqwords"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7816/7816 [09:21<00:00, 13.92it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "BeDRT2CTD639",
        "outputId": "ddc9a880-12b3-4746-ec36-cd34c524f0fd"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "      <th>Vocabular</th>\n",
              "      <th>Freq Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«Королева дерьмовых роботов» перенесла операци...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Шведская YouTube-знаменитость Симона Герц (Sim...</td>\n",
              "      <td>{'шведский': 1, 'youtube-знаменитость': 1, 'си...</td>\n",
              "      <td>[герц, девушка, врач, изобретение, опухоль]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>У Су-57 нашли «тайные ноу-хау»</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Перспективный российский многофункциональный и...</td>\n",
              "      <td>{'перспективный': 1, 'российский': 2, 'многофу...</td>\n",
              "      <td>[су, лётчик-испытатель, который, мир, толбоев]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Знарок лишился еще одного поста</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Санкт-петербургский клуб Континентальной хокке...</td>\n",
              "      <td>{'санкт-петербургский': 1, 'клуб': 2, 'контине...</td>\n",
              "      <td>[тренер, знарка, команда, ска, апрель]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>СК заинтересовался задержанием аутиста московс...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Следственный комитет по Москве начал проверку ...</td>\n",
              "      <td>{'следственный': 1, 'комитет': 1, 'москва': 1,...</td>\n",
              "      <td>[человек, задержать, иванов, молодой, молодой ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Киевляне перекрыли проспект из-за сбитого поли...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Более 100 жителей Киева перекрыли проспект Гри...</td>\n",
              "      <td>{'житель': 1, 'киев': 2, 'перекрыть': 1, 'прос...</td>\n",
              "      <td>[который, автомобиль, дтп, киев, кортеж]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Header  ...                                         Freq Words\n",
              "0  «Королева дерьмовых роботов» перенесла операци...  ...        [герц, девушка, врач, изобретение, опухоль]\n",
              "1                     У Су-57 нашли «тайные ноу-хау»  ...     [су, лётчик-испытатель, который, мир, толбоев]\n",
              "2                    Знарок лишился еще одного поста  ...             [тренер, знарка, команда, ска, апрель]\n",
              "3  СК заинтересовался задержанием аутиста московс...  ...  [человек, задержать, иванов, молодой, молодой ...\n",
              "4  Киевляне перекрыли проспект из-за сбитого поли...  ...           [который, автомобиль, дтп, киев, кортеж]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YqNBVaVD64B"
      },
      "source": [
        "Теперь попробуем другой показатель для подсчета важности слов в тексте - $TF*IDF$. Здесь $TF$ - Term Frequency, частота термина в документе, а $IDF$ - Inverted Document Frequency, обратная частота термина в коллекции (количество документов, в которых встречается данный термин).\n",
        "\n",
        "Идея метрики очень проста. Если слово встречается почти во всех документах - его различительная сила очень мала и само слово не является важным. Если слово часто встречается в данном документе, то оно являетсяя важным для него.\n",
        "\n",
        "[Иллюстрация](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-4/v-4/61)\n",
        "\n",
        "Метрика считается на коллекции документов для каждого слова, каждого документа. Для расчета меры можно использовать `TfidfVectorizer`, который работает так же как `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONBAx1ecD64D"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8kFP0CiD64D"
      },
      "source": [
        "Изменим функцию, чтобы она не возвращала часть речи. Посмотрим как TF*IDF поступает со стоп-словами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWWeAAEID64D"
      },
      "source": [
        "def normalizePymorphy2(text):\n",
        "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
        "    words = []\n",
        "    for t in tokens:\n",
        "        pv = morph.parse(t)\n",
        "        words.append(pv[0].normal_form)\n",
        "    return words    "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm72FdSHD64E"
      },
      "source": [
        "Посчитаем наиболее важные по TF*IDF слова и пары слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYavXxTGD64E"
      },
      "source": [
        "tfCounter=TfidfVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
        "tfres=tfCounter.fit_transform([' '.join(normalizePymorphy2(n)) for n in news.News] )"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgJsjszkD64E"
      },
      "source": [
        "Посчитаем пять самых важных слов по TF*IDF для каждой статьи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMznkfDVD64F",
        "outputId": "fe53eb0c-7e71-4458-a7c3-08f0ebfb8a59"
      },
      "source": [
        "freqwords = []\n",
        "for i in tqdm(range(news.shape[0])):\n",
        "\n",
        "    tfs = [(k,tfres[i][0, tfCounter.vocabulary_.get(k)]) for k in news.Vocabular.iloc[i] \n",
        "         if k in tfCounter.vocabulary_.keys()]\n",
        "    fw = [w for w, f in sorted(tfs, key = lambda x: x[1], reverse = True)[:5]]\n",
        "    freqwords.append(fw)\n",
        "    \n",
        "news['Freq Words TFIDF'] = freqwords"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7816/7816 [02:32<00:00, 51.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "ugI0--HnD64F",
        "outputId": "23ddda48-6e09-4a93-8b31-4cc0644947e5"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "      <th>Vocabular</th>\n",
              "      <th>Freq Words</th>\n",
              "      <th>Freq Words TFIDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«Королева дерьмовых роботов» перенесла операци...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Шведская YouTube-знаменитость Симона Герц (Sim...</td>\n",
              "      <td>{'шведский': 1, 'youtube-знаменитость': 1, 'си...</td>\n",
              "      <td>[герц, девушка, врач, изобретение, опухоль]</td>\n",
              "      <td>[герц, симона, изобретение, прославиться, опух...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>У Су-57 нашли «тайные ноу-хау»</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Перспективный российский многофункциональный и...</td>\n",
              "      <td>{'перспективный': 1, 'российский': 2, 'многофу...</td>\n",
              "      <td>[су, лётчик-испытатель, который, мир, толбоев]</td>\n",
              "      <td>[су, лётчик-испытатель, толбоев, кнышов, ноу-хау]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Знарок лишился еще одного поста</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Санкт-петербургский клуб Континентальной хокке...</td>\n",
              "      <td>{'санкт-петербургский': 1, 'клуб': 2, 'контине...</td>\n",
              "      <td>[тренер, знарка, команда, ска, апрель]</td>\n",
              "      <td>[знарка, ска, тренер, знарк, хоккейный]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>СК заинтересовался задержанием аутиста московс...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Следственный комитет по Москве начал проверку ...</td>\n",
              "      <td>{'следственный': 1, 'комитет': 1, 'москва': 1,...</td>\n",
              "      <td>[человек, задержать, иванов, молодой, молодой ...</td>\n",
              "      <td>[иванов, отделение, молодой человек, аутист, з...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Киевляне перекрыли проспект из-за сбитого поли...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Более 100 жителей Киева перекрыли проспект Гри...</td>\n",
              "      <td>{'житель': 1, 'киев': 2, 'перекрыть': 1, 'прос...</td>\n",
              "      <td>[который, автомобиль, дтп, киев, кортеж]</td>\n",
              "      <td>[кортеж, дтп, сбить, порошенко, григоренко]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Header  ...                                   Freq Words TFIDF\n",
              "0  «Королева дерьмовых роботов» перенесла операци...  ...  [герц, симона, изобретение, прославиться, опух...\n",
              "1                     У Су-57 нашли «тайные ноу-хау»  ...  [су, лётчик-испытатель, толбоев, кнышов, ноу-хау]\n",
              "2                    Знарок лишился еще одного поста  ...            [знарка, ска, тренер, знарк, хоккейный]\n",
              "3  СК заинтересовался задержанием аутиста московс...  ...  [иванов, отделение, молодой человек, аутист, з...\n",
              "4  Киевляне перекрыли проспект из-за сбитого поли...  ...        [кортеж, дтп, сбить, порошенко, григоренко]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kDff121D64J"
      },
      "source": [
        "### Классификация текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e17kUIehD64K"
      },
      "source": [
        "Попробуем на тех же новостях построить классификатор, отличающий новости про футбол от новостей про истребители.\n",
        "\n",
        "Для начала сформируем выборки новостей по этим темам. Истребители выделим по ключевым словам \" Су-\" и \"МиГ\", футбол - по ключевому слову \"футбол\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfgnKxS1D64K"
      },
      "source": [
        "news_air = news[news.News.str.contains(\" Су-|МиГ\")]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "_dmGWobVD64L",
        "outputId": "e22045f9-5337-41ec-9bf7-3838b275eab1"
      },
      "source": [
        "news_air.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "      <th>Vocabular</th>\n",
              "      <th>Freq Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>У Су-57 нашли «тайные ноу-хау»</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Перспективный российский многофункциональный и...</td>\n",
              "      <td>{'перспективный': 1, 'российский': 2, 'многофу...</td>\n",
              "      <td>[су, лётчик-испытатель, который, мир, толбоев]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Полет МиГ-31 в стратосфере попал на видео</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>На Камчатке летчики-истребители морской авиаци...</td>\n",
              "      <td>{'камчатка': 1, 'лётчик-истребитель': 1, 'морс...</td>\n",
              "      <td>[полёт, авиация, боевой, воскресенский, выполн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>Раскрыто вероятное имя пропавшего в Афганистан...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Найденным живым советским летчиком, пропавшим ...</td>\n",
              "      <td>{'найти': 3, 'живой': 2, 'советский': 6, 'лётч...</td>\n",
              "      <td>[афганистан, год, пропасть, советский, лётчик]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>Раскрыты неизвестные факты из биографии бывшег...</td>\n",
              "      <td>2018/06/02</td>\n",
              "      <td>Советский и российский военачальник генерал-по...</td>\n",
              "      <td>{'советский': 7, 'российский': 1, 'военачальни...</td>\n",
              "      <td>[руцкой, советский, афганистан, лётчик, год]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>Су-25 пролетел в десяти метрах над украинским ...</td>\n",
              "      <td>2018/06/02</td>\n",
              "      <td>Штурмовик Су-25 ВВС Украины пролетел на предел...</td>\n",
              "      <td>{'штурмовик': 1, 'су': 1, 'ввс': 1, 'украина':...</td>\n",
              "      <td>[азовский, азовский море, море, пограничник, у...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Header  ...                                         Freq Words\n",
              "1                       У Су-57 нашли «тайные ноу-хау»  ...     [су, лётчик-испытатель, который, мир, толбоев]\n",
              "81           Полет МиГ-31 в стратосфере попал на видео  ...  [полёт, авиация, боевой, воскресенский, выполн...\n",
              "124  Раскрыто вероятное имя пропавшего в Афганистан...  ...     [афганистан, год, пропасть, советский, лётчик]\n",
              "171  Раскрыты неизвестные факты из биографии бывшег...  ...       [руцкой, советский, афганистан, лётчик, год]\n",
              "182  Су-25 пролетел в десяти метрах над украинским ...  ...  [азовский, азовский море, море, пограничник, у...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9eleN9xD64M"
      },
      "source": [
        "news_football = news[news.News.str.contains(\"футбол\")]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "_i4HSTs7D64M",
        "outputId": "b7b13921-0b54-4a77-e6de-57620fd0bd30"
      },
      "source": [
        "news_football.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "      <th>Vocabular</th>\n",
              "      <th>Freq Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>СК заинтересовался задержанием аутиста московс...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Следственный комитет по Москве начал проверку ...</td>\n",
              "      <td>{'следственный': 1, 'комитет': 1, 'москва': 1,...</td>\n",
              "      <td>[человек, задержать, иванов, молодой, молодой ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Уткин обозвал Черчесова медной статуей</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Футбольный комментатор Василий Уткин прокоммен...</td>\n",
              "      <td>{'футбольный': 1, 'комментатор': 1, 'василий':...</td>\n",
              "      <td>[сборная, россия, сборная россия, австрия, ком...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Капитана «Реала» завалили угрозами за травму С...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Капитану мадридского «Реала» Серхио Рамосу угр...</td>\n",
              "      <td>{'капитан': 1, 'мадридский': 1, 'реал': 2, 'се...</td>\n",
              "      <td>[рамос, английский, защитник, звонок, ливерпуль]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Ученые назвали победителя ЧМ-2018 и предрекли ...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Экономисты из Инсбрукского университета (Австр...</td>\n",
              "      <td>{'экономист': 1, 'инсбрукский': 1, 'университе...</td>\n",
              "      <td>[процент, победа, чемпионат, команда, мир]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Никита Михалков объяснил происхождение своих б...</td>\n",
              "      <td>2018/06/01</td>\n",
              "      <td>Режиссер Никита Михалков рассказал, как у него...</td>\n",
              "      <td>{'режиссёр': 3, 'никита': 1, 'михалков': 3, 'р...</td>\n",
              "      <td>[год, миллион, михалков, мой, режиссёр]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Header  ...                                         Freq Words\n",
              "3   СК заинтересовался задержанием аутиста московс...  ...  [человек, задержать, иванов, молодой, молодой ...\n",
              "19             Уткин обозвал Черчесова медной статуей  ...  [сборная, россия, сборная россия, австрия, ком...\n",
              "27  Капитана «Реала» завалили угрозами за травму С...  ...   [рамос, английский, защитник, звонок, ливерпуль]\n",
              "30  Ученые назвали победителя ЧМ-2018 и предрекли ...  ...         [процент, победа, чемпионат, команда, мир]\n",
              "42  Никита Михалков объяснил происхождение своих б...  ...            [год, миллион, михалков, мой, режиссёр]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CktirMEjD64O",
        "outputId": "c794d398-ab4e-4b81-fda8-7818128a848c"
      },
      "source": [
        "news_air.shape, news_football.shape, news.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((71, 5), (1018, 5), (7816, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZt3JS-ED64Q"
      },
      "source": [
        "Выборки не сбланансированные - 1:10, нас ждут серьезные проблемы. Можно было бы сбалансировать за счет добавления новостей про авиацию вообще и жесткому отсеиванию футбола (скажем, оставить только новости в которых \"футбол\" и \"клуб\" встречается чаще всего; или оставить информацию об играх), но так интересней.\n",
        "\n",
        "Возьмем 60 новостей про истребители и 1000 про футбол, нормализуем их, посчитаем TF*IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4g5JSuZD64Q"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6pGxQ_D64R"
      },
      "source": [
        "train_size_air = 60\n",
        "train_size_football = 1000"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdF7M3HyD64T"
      },
      "source": [
        "news_train_X = np.array([' '.join(normalizePymorphy2(n)) for n in news_air.News[:train_size_air]])\n",
        "news_train_Y = np.zeros(train_size_air)\n",
        "news_train_X = np.concatenate([news_train_X, np.array([' '.join(normalizePymorphy2(n)) for n in news_football.News[:train_size_football]])])\n",
        "news_train_Y = np.concatenate([news_train_Y, np.ones(train_size_football)])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccXKZtKpD64T"
      },
      "source": [
        "news_test_X = np.array([' '.join(normalizePymorphy2(n)) for n in news_air.News[train_size_air:]])\n",
        "news_test_Y = np.zeros(news_air.shape[0] - train_size_air)\n",
        "news_test_X = np.concatenate([news_test_X, np.array([' '.join(normalizePymorphy2(n)) for n in news_football.News[train_size_football:]])])\n",
        "news_test_Y = np.concatenate([news_test_Y, np.ones(news_football.shape[0] - train_size_football)])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmXFcEfgD64V"
      },
      "source": [
        "tfCounter=TfidfVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
        "train_tf_X=tfCounter.fit_transform(news_train_X)\n",
        "test_tf_X=tfCounter.transform(news_test_X)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "732r7kZ5D64V"
      },
      "source": [
        "На сумме можно посмотреть, что новости про истребители содержат в себе гораздо меньше слов. Основное разнообразие словарю добавляет футбольная тематика. Можно серьезно сократить размер пространства за счет удаления редких слов, но тоже не сейчас."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3YAcOSlD64W",
        "outputId": "0720f7fc-48bb-4707-aacf-939559d2bb0b"
      },
      "source": [
        "np.sum(train_tf_X[:60], axis=0), np.sum(train_tf_X[60:], axis=0)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(matrix([[0.91439206, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "          0.        ]]),\n",
              " matrix([[8.64296831, 0.06857061, 0.0754369 , ..., 0.06335132, 0.03088415,\n",
              "          0.03088415]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTeG6Y4AD64X"
      },
      "source": [
        "Попробуем на этой задаче логистическую регрессию, так как она может выдавать вероятности принадлежности классам. Так как мы ждем проблемы, эта информация может нам понадобиться."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0dW-gobD64Y"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szNqhjhLD64Z",
        "outputId": "e153b02f-7c43-4b12-8c08-756f9ed3cb2c"
      },
      "source": [
        "# Берем логистическую регрессию с настройками по умолчанию.\n",
        "logreg = LogisticRegression()\n",
        "# Настраиваем ее на тренировочную выборку.\n",
        "logreg.fit(train_tf_X, news_train_Y)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDU78po6D64a"
      },
      "source": [
        "# Предсказываем результат на тестовой выборке.\n",
        "test_Y_hat = logreg.predict_proba(test_tf_X)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flO6YgPrD64c",
        "outputId": "cb738993-8dc0-428c-afa2-405e9c17afe2"
      },
      "source": [
        "# Результат посмотрим глазами.\n",
        "list(zip(test_Y_hat, news_test_Y))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([0.28613766, 0.71386234]), 0.0),\n",
              " (array([0.18893669, 0.81106331]), 0.0),\n",
              " (array([0.21797376, 0.78202624]), 0.0),\n",
              " (array([0.26702931, 0.73297069]), 0.0),\n",
              " (array([0.12671463, 0.87328537]), 0.0),\n",
              " (array([0.21754966, 0.78245034]), 0.0),\n",
              " (array([0.48896353, 0.51103647]), 0.0),\n",
              " (array([0.41647457, 0.58352543]), 0.0),\n",
              " (array([0.36164916, 0.63835084]), 0.0),\n",
              " (array([0.24866701, 0.75133299]), 0.0),\n",
              " (array([0.28608782, 0.71391218]), 0.0),\n",
              " (array([0.04434754, 0.95565246]), 1.0),\n",
              " (array([0.03125959, 0.96874041]), 1.0),\n",
              " (array([0.02785101, 0.97214899]), 1.0),\n",
              " (array([0.03433852, 0.96566148]), 1.0),\n",
              " (array([0.03980162, 0.96019838]), 1.0),\n",
              " (array([0.04437876, 0.95562124]), 1.0),\n",
              " (array([0.04272087, 0.95727913]), 1.0),\n",
              " (array([0.03525889, 0.96474111]), 1.0),\n",
              " (array([0.04662555, 0.95337445]), 1.0),\n",
              " (array([0.03408479, 0.96591521]), 1.0),\n",
              " (array([0.0308865, 0.9691135]), 1.0),\n",
              " (array([0.03483388, 0.96516612]), 1.0),\n",
              " (array([0.04642846, 0.95357154]), 1.0),\n",
              " (array([0.03877552, 0.96122448]), 1.0),\n",
              " (array([0.05049216, 0.94950784]), 1.0),\n",
              " (array([0.04000907, 0.95999093]), 1.0),\n",
              " (array([0.04561075, 0.95438925]), 1.0),\n",
              " (array([0.03078875, 0.96921125]), 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk_bFHX-D64d"
      },
      "source": [
        "Почти нигде вероятность нулевого класса не выше вероятности первого. Зато, хорошо видно, что первый класс там, где вероятность первого класса выше 0,95 или вероятность нулевого - выше 0.1. Попробуем сыграть на этом. Проверим на первых ста новостях, хорошо ли они разделяются."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCvsH-gGD64e"
      },
      "source": [
        "all_news_X = np.array([' '.join(normalizePymorphy2(n)) for n in news.News[:100]])\n",
        "all_tf_X=tfCounter.transform(all_news_X)\n",
        "all_Y_hat = logreg.predict_proba(all_tf_X)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvdOfQQnD64f",
        "outputId": "fb0d0ee0-9a4c-4fe1-d90f-10ceb5c1dea7"
      },
      "source": [
        "all_Y_hat[:10]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.052423  , 0.947577  ],\n",
              "       [0.36467998, 0.63532002],\n",
              "       [0.03777473, 0.96222527],\n",
              "       [0.04974062, 0.95025938],\n",
              "       [0.05312288, 0.94687712],\n",
              "       [0.04891462, 0.95108538],\n",
              "       [0.05035109, 0.94964891],\n",
              "       [0.05716486, 0.94283514],\n",
              "       [0.05771737, 0.94228263],\n",
              "       [0.07017849, 0.92982151]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1EAs1VgD64g",
        "outputId": "65721d4f-44bb-43ff-893d-78eb57860b62"
      },
      "source": [
        "news.Header[:100][all_Y_hat[:,0]>0.1]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1                        У Су-57 нашли «тайные ноу-хау»\n",
              "56    Российские истребители проигнорировали турок и...\n",
              "65    Сбитого 30 лет назад советского летчика нашли ...\n",
              "66                            «Центр Хруничева» усохнет\n",
              "81            Полет МиГ-31 в стратосфере попал на видео\n",
              "86    Раскрыто вероятное местонахождение сбитого во ...\n",
              "99    Производитель РД-180 признал отставание России...\n",
              "Name: Header, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93I2cvN2D64i",
        "outputId": "a490cf4b-c53a-43d0-e675-2e64543d2e42"
      },
      "source": [
        "news.Header[:100][all_Y_hat[:,1]>0.95]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2                       Знарок лишился еще одного поста\n",
              "3     СК заинтересовался задержанием аутиста московс...\n",
              "5      Российская легкоатлетка одержала 40 побед подряд\n",
              "11    Гонконгских геев полечили молитвами и отказом ...\n",
              "19               Уткин обозвал Черчесова медной статуей\n",
              "22    Школьника отказались пускать в автобус из-за н...\n",
              "24    Елена Темникова рассказала об эксплуатации сек...\n",
              "27    Капитана «Реала» завалили угрозами за травму С...\n",
              "29    Депутат заказал бизнесмена и расплатился с раз...\n",
              "30    Ученые назвали победителя ЧМ-2018 и предрекли ...\n",
              "33           Девушек в купальниках перестали фотошопить\n",
              "36    Британец заглянул под кровать и испытал радост...\n",
              "37    Проваливший чемпионат мира по хоккею тренер пр...\n",
              "38           В Канаде повадились угонять гоночные ванны\n",
              "44    Глава московского СК Дрыманов подтвердил свой ...\n",
              "46                   Зидан покинул «Реал» из-за Роналду\n",
              "52          Ubisoft анонсировала новый Assassin’s Creed\n",
              "53      Россияне забастовали после скачка цен на бензин\n",
              "67           Моуринью захотел футболиста сборной России\n",
              "68       «Лента» отказалась от принятых за лупу бутылок\n",
              "70          Темникова раскрыла причину ухода из Serebro\n",
              "73    Грузинского вора в законе избили и лишили титу...\n",
              "75    Задержанных подкидывали к потолку и пытали ток...\n",
              "79      Подмосковные жители обменяли сигареты на фрукты\n",
              "80    Плакаты к чемпионату мира по футболу возмутили...\n",
              "84    Три шутки о заднице Ким Кардашьян разозлили по...\n",
              "88    Российский журналист начал голодовку в поддерж...\n",
              "94    Зидан променяет «Реал» на многомиллионную зарп...\n",
              "Name: Header, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJYiidvzD64o",
        "outputId": "7bfef0ab-864f-45a4-a0ab-26dfd85713d1"
      },
      "source": [
        "news.Header[:100][(all_Y_hat[:,1]<0.95)&(all_Y_hat[:,0]<0.1)]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     «Королева дерьмовых роботов» перенесла операци...\n",
              "4     Киевляне перекрыли проспект из-за сбитого поли...\n",
              "6             В Google обнаружили «самый странный» сбой\n",
              "7               Ким Чен Ын предложил Путину встретиться\n",
              "8             Британка скатилась с горы и осталась жива\n",
              "                            ...                        \n",
              "93    Воробьев осмотрел новый терминал аэропорта Шер...\n",
              "95    Выступления фестиваля Primavera Sound можно бу...\n",
              "96            На Крымском мосту произошла первая авария\n",
              "97       «Русал» попробуют перевести в российский офшор\n",
              "98    Найден способ «убить» любой компьютер силой звука\n",
              "Name: Header, Length: 65, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    }
  ]
}